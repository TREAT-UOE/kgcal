{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6c81bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made model directory: 10-23_21-49-00\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import expit\n",
    "import ampligraph\n",
    "from ampligraph.latent_features import RandomBaseline, TransE, ComplEx, HolE, DistMult\n",
    "from ampligraph.datasets import load_fb13, load_wn11, load_yago3_10\n",
    "from ampligraph.utils import save_model, restore_model\n",
    "\n",
    "\n",
    "SAVE_MODEL_PATH = datetime.now().strftime('%m-%d_%H-%M-%S')\n",
    "LOAD_MODEL_PATH = ''\n",
    "os.mkdir(SAVE_MODEL_PATH)\n",
    "print('made model directory:', SAVE_MODEL_PATH)\n",
    "\n",
    "\n",
    "import calmetrics\n",
    "import calmethods\n",
    "import calutils\n",
    "\n",
    "\n",
    "class DatasetWrapper:\n",
    "    def __init__(self, name, X_train, X_valid, y_valid, X_test, y_test):\n",
    "        self.name = name\n",
    "        self.X_train = X_train\n",
    "        self.X_valid = X_valid\n",
    "        self.y_valid = y_valid\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        \n",
    "class UncalCalibtator:\n",
    "    def __init__(self):\n",
    "        self.name = 'UncalCalibtator'       \n",
    "    def fit(self, scores, y):\n",
    "        pass\n",
    "    def predict_proba(self, scores):\n",
    "        return scores\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from betacal import BetaCalibration\n",
    "\n",
    "def oneD_to_twoD(array):\n",
    "    if len(array.shape) == 1: # 1D array\n",
    "        array = array.reshape(-1, 1) # convert to 2D array\n",
    "    return array\n",
    "\n",
    "class PlattCalibtator:\n",
    "    def __name__(self):\n",
    "        return 'PlattCalibrator'\n",
    "    def __init__(self):\n",
    "        self.name = 'PlattCalibrator'\n",
    "        self._calibrator = LogisticRegression()    \n",
    "    def fit(self, uncal_probs, y):\n",
    "        uncal_probs = oneD_to_twoD(uncal_probs)\n",
    "        self._calibrator.fit(uncal_probs, y)\n",
    "        return self\n",
    "    def predict_proba(self, uncal_probs):\n",
    "        uncal_probs = oneD_to_twoD(uncal_probs)\n",
    "        return self._calibrator.predict_proba(uncal_probs)[:, 1]\n",
    "    \n",
    "class IsotonicCalibrator:\n",
    "    def __init__(self):\n",
    "        self.name = 'IsotonicCalibrator'\n",
    "        self._calibrator = IsotonicRegression(y_min=0, y_max=1, out_of_bounds='clip')\n",
    "    def fit(self, scores, y):\n",
    "        self._calibrator.fit(scores, y)\n",
    "        return self\n",
    "    def predict_proba(self, scores):\n",
    "        return self._calibrator.predict(scores)\n",
    "    \n",
    "\n",
    "class HistogramBinningCalibtator:\n",
    "    \"\"\"\n",
    "    Histogram Binning as a calibration method. The bins are divided into equal lengths.\n",
    "    \n",
    "    The class contains two methods:\n",
    "        - fit(probs, true), that should be used with validation data to train the calibration model.\n",
    "        - predict(probs), this method is used to calibrate the confidences.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, M=15):\n",
    "        \"\"\"\n",
    "        M (int): the number of equal-length bins used\n",
    "        \"\"\"\n",
    "        self.name = 'HistogramBinningCalibtator'\n",
    "        self.bin_size = 1./M  # Calculate bin size\n",
    "        self.conf = []  # Initiate confidence list\n",
    "        self.upper_bounds = np.arange(self.bin_size, 1+self.bin_size, self.bin_size)  # Set bin bounds for intervals\n",
    " \n",
    "    def _get_conf(self, conf_thresh_lower, conf_thresh_upper, probs, true):\n",
    "        \"\"\"\n",
    "        Inner method to calculate optimal confidence for certain probability range\n",
    "        \n",
    "        Params:\n",
    "            - conf_thresh_lower (float): start of the interval (not included)\n",
    "            - conf_thresh_upper (float): end of the interval (included)\n",
    "            - probs : list of probabilities.\n",
    "            - true : list with true labels, where 1 is positive class and 0 is negative).\n",
    "        \"\"\"\n",
    "\n",
    "        # Filter labels within probability range\n",
    "        filtered = [x[0] for x in zip(true, probs) if x[1] > conf_thresh_lower and x[1] <= conf_thresh_upper]\n",
    "        nr_elems = len(filtered)  # Number of elements in the list.\n",
    "\n",
    "        if nr_elems < 1:\n",
    "            return 0\n",
    "        else:\n",
    "            # In essence the confidence equals to the average accuracy of a bin\n",
    "            conf = sum(filtered)/nr_elems  # Sums positive classes\n",
    "            return conf  \n",
    "\n",
    "    def fit(self, probs, true):\n",
    "        \"\"\"\n",
    "        Fit the calibration model, finding optimal confidences for all the bins.\n",
    "        \n",
    "        Params:\n",
    "            probs: probabilities of data\n",
    "            true: true labels of data\n",
    "        \"\"\"\n",
    "\n",
    "        conf = []\n",
    "\n",
    "        # Got through intervals and add confidence to list\n",
    "        for conf_thresh in self.upper_bounds:\n",
    "            temp_conf = self._get_conf((conf_thresh - self.bin_size), conf_thresh, probs = probs, true = true)\n",
    "            conf.append(temp_conf)\n",
    "\n",
    "        self.conf = conf\n",
    "\n",
    "    # Fit based on predicted confidence\n",
    "    def predict_proba(self, probs):\n",
    "        \"\"\"\n",
    "        Calibrate the confidences\n",
    "        \n",
    "        Param:\n",
    "            probs: probabilities of the data (shape [samples, classes])\n",
    "            \n",
    "        Returns:\n",
    "            Calibrated probabilities (shape [samples, classes])\n",
    "        \"\"\"\n",
    "        # Go through all the probs and check what confidence is suitable for it.\n",
    "        for i, prob in enumerate(probs):\n",
    "            idx = np.searchsorted(self.upper_bounds, prob)\n",
    "            probs[i] = self.conf[idx]\n",
    "\n",
    "        return probs\n",
    "    \n",
    "class BetaCalibtator:\n",
    "    def __name__(self):\n",
    "        return 'BetaCalibrator'\n",
    "    def __init__(self):\n",
    "        self.name = 'BetaCalibrator'\n",
    "        self._calibrator = BetaCalibration()\n",
    "    def fit(self, uncal_probs, y):\n",
    "        self._calibrator.fit(uncal_probs, y)\n",
    "        return self\n",
    "    def predict_proba(self, uncal_probs):\n",
    "        return self._calibrator.predict(uncal_probs)\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "def load_models() -> list:\n",
    "    randm = RandomBaseline()\n",
    "    transE = TransE(verbose=True)\n",
    "    complEx = ComplEx(verbose=True)\n",
    "    distMult = DistMult(verbose=True)\n",
    "    hoLE = HolE(verbose=True)\n",
    "    return [randm, transE, complEx, distMult, hoLE]\n",
    "    \n",
    "    \n",
    "def load_yago39():\n",
    "    data = {}\n",
    "    with open('yago39/train_triple2id.txt', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        data['train'] = np.array([line.strip().split() for line in lines[1:]])\n",
    "    train_entities = set(data['train'][:, 0]).union(set(data['train'][:, 2]))\n",
    "    print(len(train_entities))\n",
    "    with open('yago39/valid_triple2id_positive.txt', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        tmp = []\n",
    "        for line in lines[1:]:\n",
    "            triple = line.strip().split()\n",
    "            if (triple[0] in train_entities) and (triple[2] in train_entities):\n",
    "                tmp.append(triple)\n",
    "        data['valid'] = np.array(tmp)\n",
    "        data['valid_labels'] = np.ones(len(tmp))\n",
    "    with open('yago39/valid_triple2id_negative.txt', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        tmp = []\n",
    "        for line in lines[1:]:\n",
    "            triple = line.strip().split()\n",
    "            if (triple[0] in train_entities) and (triple[2] in train_entities):\n",
    "                tmp.append(triple)\n",
    "        data['valid'] = np.concatenate([data['valid'], np.array(tmp)])\n",
    "        data['valid_labels'] = np.concatenate([data['valid_labels'], np.zeros(len(tmp))])\n",
    "    with open('yago39/test_triple2id_positive.txt', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        tmp = []\n",
    "        for line in lines[1:]:\n",
    "            triple = line.strip().split()\n",
    "            if (triple[0] in train_entities) and (triple[2] in train_entities):\n",
    "                tmp.append(triple)\n",
    "        data['test'] = np.array(tmp)\n",
    "        data['test_labels'] = np.ones(len(tmp))\n",
    "    with open('yago39/test_triple2id_negative.txt', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        tmp = []\n",
    "        for line in lines[1:]:\n",
    "            triple = line.strip().split()\n",
    "            if (triple[0] in train_entities) and (triple[2] in train_entities):\n",
    "                tmp.append(triple)\n",
    "        data['test'] = np.concatenate([data['test'], np.array(tmp)])\n",
    "        data['test_labels'] = np.concatenate([data['test_labels'], np.zeros(len(tmp))])\n",
    "    valid_entities = set(data['valid'][:, 0]).union(set(data['valid'][:, 2]))\n",
    "    test_entities = set(data['test'][:, 0]).union(set(data['test'][:, 2]))\n",
    "    print(len(valid_entities - train_entities))\n",
    "    print(len(test_entities - train_entities))\n",
    "    return data\n",
    "\n",
    "def load_datasets() -> list:\n",
    "    tmp = load_fb13()\n",
    "    fb13 = DatasetWrapper('FB13k', tmp['train'], \n",
    "                          tmp['valid'], \n",
    "                          tmp['valid_labels'].astype(np.int32), \n",
    "                          tmp['test'], \n",
    "                          tmp['test_labels'].astype(np.int32))\n",
    "    \n",
    "    tmp = load_wn11()\n",
    "    wn11 = DatasetWrapper('WN11', tmp['train'], \n",
    "                          tmp['valid'], \n",
    "                          tmp['valid_labels'].astype(np.int32), \n",
    "                          tmp['test'], \n",
    "                          tmp['test_labels'].astype(np.int32))\n",
    "    \n",
    "    tmp = load_yago39()\n",
    "    yago39 = DatasetWrapper('YAGO39', tmp['train'], \n",
    "                          tmp['valid'], \n",
    "                          tmp['valid_labels'].astype(np.int32), \n",
    "                          tmp['test'], \n",
    "                          tmp['test_labels'].astype(np.int32))\n",
    "    \n",
    "    return [fb13, wn11, yago39]\n",
    "\n",
    "def load_calibrators() -> list:\n",
    "    uncal = UncalCalibtator()\n",
    "    platt = PlattCalibtator()\n",
    "    isot = IsotonicCalibrator()\n",
    "    histbin = HistogramBinningCalibtator()\n",
    "    beta = BetaCalibtator()\n",
    "    return [uncal, platt, beta, isot, histbin]\n",
    "\n",
    "def load_metrics() -> list:\n",
    "    return [\n",
    "        calmetrics.brier_score,\n",
    "        calmetrics.negative_log_loss,\n",
    "        calmetrics.ks_error\n",
    "    ]\n",
    "\n",
    "    \n",
    "def train_and_eval(model, dataset, calibrators, func_metrics):\n",
    "    # train model with X_train\n",
    "    print(f'training model {model.__class__.__name__} for dataset {dataset.name} ...')\n",
    "    model.fit(dataset.X_train)\n",
    "    save_model(model, model_name_path=os.path.join(SAVE_MODEL_PATH, f'{model.__class__.__name__}-{dataset.name}.pkl'))\n",
    "    # train calibrators \n",
    "    scores_valid = expit(model.predict(dataset.X_valid))\n",
    "    scores_test = expit(model.predict(dataset.X_test))\n",
    "    for calibrator in calibrators:\n",
    "        print(f'training calibrator {calibrator.name} for  model {model.__class__.__name__} and dataset {dataset.name} ...')\n",
    "        calibrator.fit(scores_valid, dataset.y_valid)\n",
    "        probs_test = calibrator.predict_proba(scores_test)\n",
    "        for func_metric in func_metrics:\n",
    "            print(func_metric.__name__, \":\", func_metric(dataset.y_test, probs_test))\n",
    "    \n",
    "def main():\n",
    "    \n",
    "    models = load_models()\n",
    "    datasets = load_datasets()\n",
    "    calibrators = load_calibrators()\n",
    "    func_metrics = load_metrics()\n",
    "    \n",
    "    from copy import deepcopy\n",
    "    for model in models:\n",
    "        for dataset in datasets:\n",
    "            try:\n",
    "                blank_model = deepcopy(model)\n",
    "                blank_calibrators = deepcopy(calibrators)\n",
    "                train_and_eval(blank_model, dataset, blank_calibrators, func_metrics)\n",
    "                print('\\n-------------------------------------\\n')\n",
    "            except Exception as e:\n",
    "                print('Error:', e)\n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f42d018a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING - All triples will be processed in the same batch (batches_count=1). When processing large graphs it is recommended to batch the input knowledge graph instead.\n",
      "32079\n",
      "0\n",
      "0\n",
      "training model RandomBaseline for dataset FB13k ...\n",
      "training calibrator UncalCalibtator for  model RandomBaseline and dataset FB13k ...\n",
      "brier_score : 0.2684975284759183\n",
      "negative_log_loss : 0.7328676969634227\n",
      "ks_error : 0.12044093177066617\n",
      "training calibrator PlattCalibrator for  model RandomBaseline and dataset FB13k ...\n",
      "brier_score : 0.24999518938895152\n",
      "negative_log_loss : 0.6931375648878926\n",
      "ks_error : 0.0020835739842205436\n",
      "training calibrator BetaCalibrator for  model RandomBaseline and dataset FB13k ...\n",
      "brier_score : 0.250000257510694\n",
      "negative_log_loss : 0.6931477050373447\n",
      "ks_error : 0.0024444005960202464\n",
      "training calibrator IsotonicCalibrator for  model RandomBaseline and dataset FB13k ...\n",
      "brier_score : 0.2501517026620063\n",
      "negative_log_loss : 0.6935005047487761\n",
      "ks_error : 0.0039150408361927624\n",
      "training calibrator HistogramBinningCalibtator for  model RandomBaseline and dataset FB13k ...\n",
      "brier_score : 0.2500416275682394\n",
      "negative_log_loss : 0.6932304549303622\n",
      "ks_error : 0.00441278100827483\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "training model RandomBaseline for dataset WN11 ...\n",
      "training calibrator UncalCalibtator for  model RandomBaseline and dataset WN11 ...\n",
      "brier_score : 0.27190565490760077\n",
      "negative_log_loss : 0.740045067783807\n",
      "ks_error : 0.12745471454012713\n",
      "training calibrator PlattCalibrator for  model RandomBaseline and dataset WN11 ...\n",
      "brier_score : 0.25012712073605964\n",
      "negative_log_loss : 0.6934015300730384\n",
      "ks_error : 0.007668407820740825\n",
      "training calibrator BetaCalibrator for  model RandomBaseline and dataset WN11 ...\n",
      "brier_score : 0.25018475230638365\n",
      "negative_log_loss : 0.6935168526119463\n",
      "ks_error : 0.008427124083325604\n",
      "training calibrator IsotonicCalibrator for  model RandomBaseline and dataset WN11 ...\n",
      "brier_score : 0.2507221439437313\n",
      "negative_log_loss : 0.7011661107096953\n",
      "ks_error : 0.007158510247807837\n",
      "training calibrator HistogramBinningCalibtator for  model RandomBaseline and dataset WN11 ...\n",
      "brier_score : 0.2501246580342697\n",
      "negative_log_loss : 0.6933967061246287\n",
      "ks_error : 0.005898516183938496\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "training model RandomBaseline for dataset YAGO39 ...\n",
      "ERROR - Input triples include one or more concepts not present in the training set. Please filter all concepts in X that do not occur in the training test (set filter_unseen=True in evaluate_performance) or retrain the model on a training set that includes all the desired concept types.\n",
      "Error: Input triples include one or more concepts not present in the training set. Please filter all concepts in X that do not occur in the training test (set filter_unseen=True in evaluate_performance) or retrain the model on a training set that includes all the desired concept types.\n",
      "training model TransE for dataset FB13k ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average TransE Loss:   1.087920: 100%|████████████████████████████████████████████| 100/100 [17:21<00:00, 10.41s/epoch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training calibrator UncalCalibtator for  model TransE and dataset FB13k ...\n",
      "brier_score : 0.24203960267948016\n",
      "negative_log_loss : 0.6761335508856489\n",
      "ks_error : 0.09865471556438632\n",
      "training calibrator PlattCalibrator for  model TransE and dataset FB13k ...\n",
      "brier_score : 0.21235960241267351\n",
      "negative_log_loss : 0.6165714271049767\n",
      "ks_error : 0.028312188456209808\n",
      "training calibrator BetaCalibrator for  model TransE and dataset FB13k ...\n",
      "brier_score : 0.20924600244971653\n",
      "negative_log_loss : 0.6099175821122884\n",
      "ks_error : 0.01862409469581283\n",
      "training calibrator IsotonicCalibrator for  model TransE and dataset FB13k ...\n",
      "brier_score : 0.20591321375918967\n",
      "negative_log_loss : nan\n",
      "ks_error : 0.004375228652981428\n",
      "training calibrator HistogramBinningCalibtator for  model TransE and dataset FB13k ...\n",
      "brier_score : 0.21607712151624078\n",
      "negative_log_loss : 0.6204131372482057\n",
      "ks_error : 0.0033984795129863854\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "training model TransE for dataset WN11 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average TransE Loss:   0.961199: 100%|████████████████████████████████████████████| 100/100 [07:28<00:00,  4.48s/epoch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training calibrator UncalCalibtator for  model TransE and dataset WN11 ...\n",
      "brier_score : 0.24231852583366786\n",
      "negative_log_loss : 0.7020343918818709\n",
      "ks_error : 0.3103566750253407\n",
      "training calibrator PlattCalibrator for  model TransE and dataset WN11 ...\n",
      "brier_score : 0.09035140122217608\n",
      "negative_log_loss : 0.30856702474268194\n",
      "ks_error : 0.024178158455601114\n",
      "training calibrator BetaCalibrator for  model TransE and dataset WN11 ...\n",
      "brier_score : 0.09010570081884385\n",
      "negative_log_loss : 0.3090112773520534\n",
      "ks_error : 0.018783613764751874\n",
      "training calibrator IsotonicCalibrator for  model TransE and dataset WN11 ...\n",
      "brier_score : 0.08738154675067811\n",
      "negative_log_loss : nan\n",
      "ks_error : 0.004423032990856897\n",
      "training calibrator HistogramBinningCalibtator for  model TransE and dataset WN11 ...\n",
      "brier_score : 0.08826893223426378\n",
      "negative_log_loss : nan\n",
      "ks_error : 0.005896101671796661\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "training model TransE for dataset YAGO39 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average TransE Loss:   0.908316: 100%|████████████████████████████████████████████| 100/100 [12:12<00:00,  7.33s/epoch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR - Input triples include one or more concepts not present in the training set. Please filter all concepts in X that do not occur in the training test (set filter_unseen=True in evaluate_performance) or retrain the model on a training set that includes all the desired concept types.\n",
      "Error: Input triples include one or more concepts not present in the training set. Please filter all concepts in X that do not occur in the training test (set filter_unseen=True in evaluate_performance) or retrain the model on a training set that includes all the desired concept types.\n",
      "training model ComplEx for dataset FB13k ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average ComplEx Loss:   0.191861: 100%|███████████████████████████████████████████| 100/100 [39:54<00:00, 23.94s/epoch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training calibrator UncalCalibtator for  model ComplEx and dataset FB13k ...\n",
      "brier_score : 0.4100634233743666\n",
      "negative_log_loss : 2.2903791421950426\n",
      "ks_error : 0.4220453333087641\n",
      "training calibrator PlattCalibrator for  model ComplEx and dataset FB13k ...\n",
      "brier_score : 0.2211132503262728\n",
      "negative_log_loss : 0.6332010154100297\n",
      "ks_error : 0.05315326683122401\n",
      "training calibrator BetaCalibrator for  model ComplEx and dataset FB13k ...\n",
      "brier_score : 0.22553111302435333\n",
      "negative_log_loss : 0.6423298893077655\n",
      "ks_error : 0.05031979974985035\n",
      "training calibrator IsotonicCalibrator for  model ComplEx and dataset FB13k ...\n",
      "brier_score : 0.20521510205128568\n",
      "negative_log_loss : nan\n",
      "ks_error : 0.004743598488195722\n",
      "training calibrator HistogramBinningCalibtator for  model ComplEx and dataset FB13k ...\n",
      "brier_score : 0.22315979532144595\n",
      "negative_log_loss : 0.6330021293015236\n",
      "ks_error : 0.002960665826148068\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "training model ComplEx for dataset WN11 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average ComplEx Loss:   0.008186: 100%|███████████████████████████████████████████| 100/100 [16:55<00:00, 10.16s/epoch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training calibrator UncalCalibtator for  model ComplEx and dataset WN11 ...\n",
      "brier_score : 0.4198317154303649\n",
      "negative_log_loss : 2.8128136065043825\n",
      "ks_error : 0.43282180679358045\n",
      "training calibrator PlattCalibrator for  model ComplEx and dataset WN11 ...\n",
      "brier_score : 0.2259155996626409\n",
      "negative_log_loss : 0.6357144121708104\n",
      "ks_error : 0.016950530064222202\n",
      "training calibrator BetaCalibrator for  model ComplEx and dataset WN11 ...\n",
      "brier_score : 0.22720049101997886\n",
      "negative_log_loss : 0.6444555697363239\n",
      "ks_error : 0.015109823072445738\n",
      "training calibrator IsotonicCalibrator for  model ComplEx and dataset WN11 ...\n",
      "brier_score : 0.22403506490499536\n",
      "negative_log_loss : 0.6308302582142064\n",
      "ks_error : 0.0025818758313999185\n",
      "training calibrator HistogramBinningCalibtator for  model ComplEx and dataset WN11 ...\n",
      "brier_score : 0.22619791611170936\n",
      "negative_log_loss : nan\n",
      "ks_error : 0.0025793498970464457\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "training model ComplEx for dataset YAGO39 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average ComplEx Loss:   0.067053: 100%|███████████████████████████████████████████| 100/100 [32:33<00:00, 19.53s/epoch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR - Input triples include one or more concepts not present in the training set. Please filter all concepts in X that do not occur in the training test (set filter_unseen=True in evaluate_performance) or retrain the model on a training set that includes all the desired concept types.\n",
      "Error: Input triples include one or more concepts not present in the training set. Please filter all concepts in X that do not occur in the training test (set filter_unseen=True in evaluate_performance) or retrain the model on a training set that includes all the desired concept types.\n",
      "training model DistMult for dataset FB13k ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average DistMult Loss:   0.213265: 100%|██████████████████████████████████████████| 100/100 [18:12<00:00, 10.93s/epoch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training calibrator UncalCalibtator for  model DistMult and dataset FB13k ...\n",
      "brier_score : 0.3836241201356534\n",
      "negative_log_loss : 2.01791222113999\n",
      "ks_error : 0.3802549206957374\n",
      "training calibrator PlattCalibrator for  model DistMult and dataset FB13k ...\n",
      "brier_score : 0.22967954497012896\n",
      "negative_log_loss : 0.6465461397346891\n",
      "ks_error : 0.023676875749636228\n",
      "training calibrator BetaCalibrator for  model DistMult and dataset FB13k ...\n",
      "brier_score : 0.23052820336624266\n",
      "negative_log_loss : 0.6495942738889137\n",
      "ks_error : 0.031198408051960336\n",
      "training calibrator IsotonicCalibrator for  model DistMult and dataset FB13k ...\n",
      "brier_score : 0.22520472193937432\n",
      "negative_log_loss : nan\n",
      "ks_error : 0.0034514320965623457\n",
      "training calibrator HistogramBinningCalibtator for  model DistMult and dataset FB13k ...\n",
      "brier_score : 0.2288702236083335\n",
      "negative_log_loss : 0.6448424065555999\n",
      "ks_error : 0.002858426204001785\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "training model DistMult for dataset WN11 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average DistMult Loss:   0.028390: 100%|██████████████████████████████████████████| 100/100 [07:53<00:00,  4.73s/epoch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training calibrator UncalCalibtator for  model DistMult and dataset WN11 ...\n",
      "brier_score : 0.3922234910240632\n",
      "negative_log_loss : 2.0934008919659868\n",
      "ks_error : 0.40822910323193096\n",
      "training calibrator PlattCalibrator for  model DistMult and dataset WN11 ...\n",
      "brier_score : 0.2184343561167011\n",
      "negative_log_loss : 0.6179961966044359\n",
      "ks_error : 0.014669746067275335\n",
      "training calibrator BetaCalibrator for  model DistMult and dataset WN11 ...\n",
      "brier_score : 0.218962545442956\n",
      "negative_log_loss : 0.6260590679025031\n",
      "ks_error : 0.01555612355144076\n",
      "training calibrator IsotonicCalibrator for  model DistMult and dataset WN11 ...\n",
      "brier_score : 0.217461666409126\n",
      "negative_log_loss : nan\n",
      "ks_error : 0.003814560609893214\n",
      "training calibrator HistogramBinningCalibtator for  model DistMult and dataset WN11 ...\n",
      "brier_score : 0.2185885698941902\n",
      "negative_log_loss : nan\n",
      "ks_error : 0.00350753810397747\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "training model DistMult for dataset YAGO39 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average DistMult Loss:   0.099399: 100%|██████████████████████████████████████████| 100/100 [12:18<00:00,  7.39s/epoch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR - Input triples include one or more concepts not present in the training set. Please filter all concepts in X that do not occur in the training test (set filter_unseen=True in evaluate_performance) or retrain the model on a training set that includes all the desired concept types.\n",
      "Error: Input triples include one or more concepts not present in the training set. Please filter all concepts in X that do not occur in the training test (set filter_unseen=True in evaluate_performance) or retrain the model on a training set that includes all the desired concept types.\n",
      "training model HolE for dataset FB13k ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average HolE Loss:   0.725869: 100%|██████████████████████████████████████████████| 100/100 [39:35<00:00, 23.75s/epoch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training calibrator UncalCalibtator for  model HolE and dataset FB13k ...\n",
      "brier_score : 0.31094050467705275\n",
      "negative_log_loss : 0.9264620093097848\n",
      "ks_error : 0.16975161457582166\n",
      "training calibrator PlattCalibrator for  model HolE and dataset FB13k ...\n",
      "brier_score : 0.2480769873583764\n",
      "negative_log_loss : 0.689249909530124\n",
      "ks_error : 0.06293461663932887\n",
      "training calibrator BetaCalibrator for  model HolE and dataset FB13k ...\n",
      "brier_score : 0.2348090436520992\n",
      "negative_log_loss : 0.65813781970812\n",
      "ks_error : 0.07901435864140946\n",
      "training calibrator IsotonicCalibrator for  model HolE and dataset FB13k ...\n",
      "brier_score : 0.21655216019360604\n",
      "negative_log_loss : nan\n",
      "ks_error : 0.0026453995551947163\n",
      "training calibrator HistogramBinningCalibtator for  model HolE and dataset FB13k ...\n",
      "brier_score : 0.17979725676797267\n",
      "negative_log_loss : 0.5369826244657645\n",
      "ks_error : 0.005258060242059603\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "training model HolE for dataset WN11 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average HolE Loss:   0.723459: 100%|██████████████████████████████████████████████| 100/100 [16:52<00:00, 10.13s/epoch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training calibrator UncalCalibtator for  model HolE and dataset WN11 ...\n",
      "brier_score : 0.2083094776375876\n",
      "negative_log_loss : 0.593846216900172\n",
      "ks_error : 0.09535406240893779\n",
      "training calibrator PlattCalibrator for  model HolE and dataset WN11 ...\n",
      "brier_score : 0.1986524832227268\n",
      "negative_log_loss : 0.5762679173345209\n",
      "ks_error : 0.022063785493188826\n",
      "training calibrator BetaCalibrator for  model HolE and dataset WN11 ...\n",
      "brier_score : 0.19505010721568744\n",
      "negative_log_loss : 0.5632258837783274\n",
      "ks_error : 0.013829448549426693\n",
      "training calibrator IsotonicCalibrator for  model HolE and dataset WN11 ...\n",
      "brier_score : 0.19226344814198223\n",
      "negative_log_loss : 0.5528811532586023\n",
      "ks_error : 0.0032608614020646076\n",
      "training calibrator HistogramBinningCalibtator for  model HolE and dataset WN11 ...\n",
      "brier_score : 0.19288603268089713\n",
      "negative_log_loss : nan\n",
      "ks_error : 0.005407610596418033\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "training model HolE for dataset YAGO39 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average HolE Loss:   0.330359: 100%|██████████████████████████████████████████████| 100/100 [31:43<00:00, 19.03s/epoch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR - Input triples include one or more concepts not present in the training set. Please filter all concepts in X that do not occur in the training test (set filter_unseen=True in evaluate_performance) or retrain the model on a training set that includes all the desired concept types.\n",
      "Error: Input triples include one or more concepts not present in the training set. Please filter all concepts in X that do not occur in the training test (set filter_unseen=True in evaluate_performance) or retrain the model on a training set that includes all the desired concept types.\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1746dac5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1f434ca",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32079\n",
      "0\n",
      "0\n",
      "WARNING - All triples will be processed in the same batch (batches_count=1). When processing large graphs it is recommended to batch the input knowledge graph instead.\n",
      "WARNING - All triples will be processed in the same batch (batches_count=1). When processing large graphs it is recommended to batch the input knowledge graph instead.\n",
      "WARNING - All triples will be processed in the same batch (batches_count=1). When processing large graphs it is recommended to batch the input knowledge graph instead.\n",
      "training calibrator UncalCalibtator for  model RandomBaseline and dataset FB13k ...\n",
      "brier_score         : 0.268797\n",
      "negative_log_loss   : 0.733519\n",
      "ks_error            : 0.119869\n",
      "training calibrator PlattCalibrator for  model RandomBaseline and dataset FB13k ...\n",
      "brier_score         : 0.250000\n",
      "negative_log_loss   : 0.693147\n",
      "ks_error            : 0.001434\n",
      "training calibrator BetaCalibrator for  model RandomBaseline and dataset FB13k ...\n",
      "brier_score         : 0.250000\n",
      "negative_log_loss   : 0.693147\n",
      "ks_error            : 0.001401\n",
      "training calibrator IsotonicCalibrator for  model RandomBaseline and dataset FB13k ...\n",
      "brier_score         : 0.250247\n",
      "negative_log_loss   : 0.699733\n",
      "ks_error            : 0.001917\n",
      "training calibrator HistogramBinningCalibtator for  model RandomBaseline and dataset FB13k ...\n",
      "brier_score         : 0.250020\n",
      "negative_log_loss   : 0.693187\n",
      "ks_error            : 0.002615\n",
      "                      Uncal     Platt      Beta  Isotonic  HistogramBinning\n",
      "brier_score        0.268797  0.250000  0.250000  0.250247          0.250020\n",
      "negative_log_loss  0.733519  0.693147  0.693147  0.699733          0.693187\n",
      "ks_error           0.119869  0.001434  0.001401  0.001917          0.002615\n",
      "\n",
      "----------------------------------\n",
      "\n",
      "training calibrator UncalCalibtator for  model RandomBaseline and dataset WN11 ...\n",
      "brier_score         : 0.270669\n",
      "negative_log_loss   : 0.737431\n",
      "ks_error            : 0.127916\n",
      "training calibrator PlattCalibrator for  model RandomBaseline and dataset WN11 ...\n",
      "brier_score         : 0.250069\n",
      "negative_log_loss   : 0.693284\n",
      "ks_error            : 0.006887\n",
      "training calibrator BetaCalibrator for  model RandomBaseline and dataset WN11 ...\n",
      "brier_score         : 0.250132\n",
      "negative_log_loss   : 0.693411\n",
      "ks_error            : 0.007947\n",
      "training calibrator IsotonicCalibrator for  model RandomBaseline and dataset WN11 ...\n",
      "brier_score         : 0.250832\n",
      "negative_log_loss   : 0.698312\n",
      "ks_error            : 0.009895\n",
      "training calibrator HistogramBinningCalibtator for  model RandomBaseline and dataset WN11 ...\n",
      "brier_score         : 0.250208\n",
      "negative_log_loss   : 0.693565\n",
      "ks_error            : 0.006493\n",
      "                      Uncal     Platt      Beta  Isotonic  HistogramBinning\n",
      "brier_score        0.270669  0.250069  0.250132  0.250832          0.250208\n",
      "negative_log_loss  0.737431  0.693284  0.693411  0.698312          0.693565\n",
      "ks_error           0.127916  0.006887  0.007947  0.009895          0.006493\n",
      "\n",
      "----------------------------------\n",
      "\n",
      "training calibrator UncalCalibtator for  model TransE and dataset FB13k ...\n",
      "brier_score         : 0.242040\n",
      "negative_log_loss   : 0.676134\n",
      "ks_error            : 0.098655\n",
      "training calibrator PlattCalibrator for  model TransE and dataset FB13k ...\n",
      "brier_score         : 0.212360\n",
      "negative_log_loss   : 0.616571\n",
      "ks_error            : 0.028312\n",
      "training calibrator BetaCalibrator for  model TransE and dataset FB13k ...\n",
      "brier_score         : 0.209246\n",
      "negative_log_loss   : 0.609918\n",
      "ks_error            : 0.018624\n",
      "training calibrator IsotonicCalibrator for  model TransE and dataset FB13k ...\n",
      "brier_score         : 0.205913\n",
      "negative_log_loss   : 0.597525\n",
      "ks_error            : 0.004375\n",
      "training calibrator HistogramBinningCalibtator for  model TransE and dataset FB13k ...\n",
      "brier_score         : 0.216077\n",
      "negative_log_loss   : 0.620413\n",
      "ks_error            : 0.003398\n",
      "                      Uncal     Platt      Beta  Isotonic  HistogramBinning\n",
      "brier_score        0.242040  0.212360  0.209246  0.205913          0.216077\n",
      "negative_log_loss  0.676134  0.616571  0.609918  0.597525          0.620413\n",
      "ks_error           0.098655  0.028312  0.018624  0.004375          0.003398\n",
      "\n",
      "----------------------------------\n",
      "\n",
      "training calibrator UncalCalibtator for  model TransE and dataset WN11 ...\n",
      "brier_score         : 0.242319\n",
      "negative_log_loss   : 0.702034\n",
      "ks_error            : 0.310357\n",
      "training calibrator PlattCalibrator for  model TransE and dataset WN11 ...\n",
      "brier_score         : 0.090351\n",
      "negative_log_loss   : 0.308567\n",
      "ks_error            : 0.024178\n",
      "training calibrator BetaCalibrator for  model TransE and dataset WN11 ...\n",
      "brier_score         : 0.090106\n",
      "negative_log_loss   : 0.309011\n",
      "ks_error            : 0.018784\n",
      "training calibrator IsotonicCalibrator for  model TransE and dataset WN11 ...\n",
      "brier_score         : 0.087382\n",
      "negative_log_loss   : 0.298512\n",
      "ks_error            : 0.004423\n",
      "training calibrator HistogramBinningCalibtator for  model TransE and dataset WN11 ...\n",
      "brier_score         : 0.088269\n",
      "negative_log_loss   : 0.301354\n",
      "ks_error            : 0.005896\n",
      "                      Uncal     Platt      Beta  Isotonic  HistogramBinning\n",
      "brier_score        0.242319  0.090351  0.090106  0.087382          0.088269\n",
      "negative_log_loss  0.702034  0.308567  0.309011  0.298512          0.301354\n",
      "ks_error           0.310357  0.024178  0.018784  0.004423          0.005896\n",
      "\n",
      "----------------------------------\n",
      "\n",
      "training calibrator UncalCalibtator for  model ComplEx and dataset FB13k ...\n",
      "brier_score         : 0.410063\n",
      "negative_log_loss   : 2.283551\n",
      "ks_error            : 0.422045\n",
      "training calibrator PlattCalibrator for  model ComplEx and dataset FB13k ...\n",
      "brier_score         : 0.221113\n",
      "negative_log_loss   : 0.633201\n",
      "ks_error            : 0.053153\n",
      "training calibrator BetaCalibrator for  model ComplEx and dataset FB13k ...\n",
      "brier_score         : 0.225531\n",
      "negative_log_loss   : 0.642330\n",
      "ks_error            : 0.050320\n",
      "training calibrator IsotonicCalibrator for  model ComplEx and dataset FB13k ...\n",
      "brier_score         : 0.205215\n",
      "negative_log_loss   : 0.596296\n",
      "ks_error            : 0.004744\n",
      "training calibrator HistogramBinningCalibtator for  model ComplEx and dataset FB13k ...\n",
      "brier_score         : 0.223160\n",
      "negative_log_loss   : 0.633002\n",
      "ks_error            : 0.002961\n",
      "                      Uncal     Platt      Beta  Isotonic  HistogramBinning\n",
      "brier_score        0.410063  0.221113  0.225531  0.205215          0.223160\n",
      "negative_log_loss  2.283551  0.633201  0.642330  0.596296          0.633002\n",
      "ks_error           0.422045  0.053153  0.050320  0.004744          0.002961\n",
      "\n",
      "----------------------------------\n",
      "\n",
      "training calibrator UncalCalibtator for  model ComplEx and dataset WN11 ...\n",
      "brier_score         : 0.419832\n",
      "negative_log_loss   : 2.811955\n",
      "ks_error            : 0.432822\n",
      "training calibrator PlattCalibrator for  model ComplEx and dataset WN11 ...\n",
      "brier_score         : 0.225916\n",
      "negative_log_loss   : 0.635714\n",
      "ks_error            : 0.016951\n",
      "training calibrator BetaCalibrator for  model ComplEx and dataset WN11 ...\n",
      "brier_score         : 0.227200\n",
      "negative_log_loss   : 0.642309\n",
      "ks_error            : 0.015110\n",
      "training calibrator IsotonicCalibrator for  model ComplEx and dataset WN11 ...\n",
      "brier_score         : 0.224035\n",
      "negative_log_loss   : 0.630830\n",
      "ks_error            : 0.002582\n",
      "training calibrator HistogramBinningCalibtator for  model ComplEx and dataset WN11 ...\n",
      "brier_score         : 0.226198\n",
      "negative_log_loss   : 0.638947\n",
      "ks_error            : 0.002579\n",
      "                      Uncal     Platt      Beta  Isotonic  HistogramBinning\n",
      "brier_score        0.419832  0.225916  0.227200  0.224035          0.226198\n",
      "negative_log_loss  2.811955  0.635714  0.642309  0.630830          0.638947\n",
      "ks_error           0.432822  0.016951  0.015110  0.002582          0.002579\n",
      "\n",
      "----------------------------------\n",
      "\n",
      "training calibrator UncalCalibtator for  model DistMult and dataset FB13k ...\n",
      "brier_score         : 0.383624\n",
      "negative_log_loss   : 2.012092\n",
      "ks_error            : 0.380255\n",
      "training calibrator PlattCalibrator for  model DistMult and dataset FB13k ...\n",
      "brier_score         : 0.229680\n",
      "negative_log_loss   : 0.646546\n",
      "ks_error            : 0.023677\n",
      "training calibrator BetaCalibrator for  model DistMult and dataset FB13k ...\n",
      "brier_score         : 0.230528\n",
      "negative_log_loss   : 0.649594\n",
      "ks_error            : 0.031198\n",
      "training calibrator IsotonicCalibrator for  model DistMult and dataset FB13k ...\n",
      "brier_score         : 0.225205\n",
      "negative_log_loss   : 0.637874\n",
      "ks_error            : 0.003451\n",
      "training calibrator HistogramBinningCalibtator for  model DistMult and dataset FB13k ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brier_score         : 0.228870\n",
      "negative_log_loss   : 0.644842\n",
      "ks_error            : 0.002858\n",
      "                      Uncal     Platt      Beta  Isotonic  HistogramBinning\n",
      "brier_score        0.383624  0.229680  0.230528  0.225205          0.228870\n",
      "negative_log_loss  2.012092  0.646546  0.649594  0.637874          0.644842\n",
      "ks_error           0.380255  0.023677  0.031198  0.003451          0.002858\n",
      "\n",
      "----------------------------------\n",
      "\n",
      "training calibrator UncalCalibtator for  model DistMult and dataset WN11 ...\n",
      "brier_score         : 0.392223\n",
      "negative_log_loss   : 2.093234\n",
      "ks_error            : 0.408229\n",
      "training calibrator PlattCalibrator for  model DistMult and dataset WN11 ...\n",
      "brier_score         : 0.218434\n",
      "negative_log_loss   : 0.617996\n",
      "ks_error            : 0.014670\n",
      "training calibrator BetaCalibrator for  model DistMult and dataset WN11 ...\n",
      "brier_score         : 0.218963\n",
      "negative_log_loss   : 0.622969\n",
      "ks_error            : 0.015556\n",
      "training calibrator IsotonicCalibrator for  model DistMult and dataset WN11 ...\n",
      "brier_score         : 0.217462\n",
      "negative_log_loss   : 0.616110\n",
      "ks_error            : 0.003815\n",
      "training calibrator HistogramBinningCalibtator for  model DistMult and dataset WN11 ...\n",
      "brier_score         : 0.218589\n",
      "negative_log_loss   : 0.619896\n",
      "ks_error            : 0.003508\n",
      "                      Uncal     Platt      Beta  Isotonic  HistogramBinning\n",
      "brier_score        0.392223  0.218434  0.218963  0.217462          0.218589\n",
      "negative_log_loss  2.093234  0.617996  0.622969  0.616110          0.619896\n",
      "ks_error           0.408229  0.014670  0.015556  0.003815          0.003508\n",
      "\n",
      "----------------------------------\n",
      "\n",
      "training calibrator UncalCalibtator for  model HolE and dataset FB13k ...\n",
      "brier_score         : 0.310941\n",
      "negative_log_loss   : 0.926462\n",
      "ks_error            : 0.169752\n",
      "training calibrator PlattCalibrator for  model HolE and dataset FB13k ...\n",
      "brier_score         : 0.248077\n",
      "negative_log_loss   : 0.689250\n",
      "ks_error            : 0.062935\n",
      "training calibrator BetaCalibrator for  model HolE and dataset FB13k ...\n",
      "brier_score         : 0.234809\n",
      "negative_log_loss   : 0.658138\n",
      "ks_error            : 0.079014\n",
      "training calibrator IsotonicCalibrator for  model HolE and dataset FB13k ...\n",
      "brier_score         : 0.216552\n",
      "negative_log_loss   : 0.616338\n",
      "ks_error            : 0.002645\n",
      "training calibrator HistogramBinningCalibtator for  model HolE and dataset FB13k ...\n",
      "brier_score         : 0.179797\n",
      "negative_log_loss   : 0.536983\n",
      "ks_error            : 0.005258\n",
      "                      Uncal     Platt      Beta  Isotonic  HistogramBinning\n",
      "brier_score        0.310941  0.248077  0.234809  0.216552          0.179797\n",
      "negative_log_loss  0.926462  0.689250  0.658138  0.616338          0.536983\n",
      "ks_error           0.169752  0.062935  0.079014  0.002645          0.005258\n",
      "\n",
      "----------------------------------\n",
      "\n",
      "training calibrator UncalCalibtator for  model HolE and dataset WN11 ...\n",
      "brier_score         : 0.208309\n",
      "negative_log_loss   : 0.593846\n",
      "ks_error            : 0.095354\n",
      "training calibrator PlattCalibrator for  model HolE and dataset WN11 ...\n",
      "brier_score         : 0.198652\n",
      "negative_log_loss   : 0.576268\n",
      "ks_error            : 0.022064\n",
      "training calibrator BetaCalibrator for  model HolE and dataset WN11 ...\n",
      "brier_score         : 0.195050\n",
      "negative_log_loss   : 0.563226\n",
      "ks_error            : 0.013829\n",
      "training calibrator IsotonicCalibrator for  model HolE and dataset WN11 ...\n",
      "brier_score         : 0.192263\n",
      "negative_log_loss   : 0.552881\n",
      "ks_error            : 0.003261\n",
      "training calibrator HistogramBinningCalibtator for  model HolE and dataset WN11 ...\n",
      "brier_score         : 0.192886\n",
      "negative_log_loss   : 0.557150\n",
      "ks_error            : 0.005408\n",
      "                      Uncal     Platt      Beta  Isotonic  HistogramBinning\n",
      "brier_score        0.208309  0.198652  0.195050  0.192263          0.192886\n",
      "negative_log_loss  0.593846  0.576268  0.563226  0.552881          0.557150\n",
      "ks_error           0.095354  0.022064  0.013829  0.003261          0.005408\n",
      "\n",
      "----------------------------------\n",
      "\n",
      "training calibrator UncalCalibtator for  model TransE and dataset FB13k ...\n",
      "brier_score         : 0.242040\n",
      "negative_log_loss   : 0.676134\n",
      "ks_error            : 0.098655\n",
      "training calibrator PlattCalibrator for  model TransE and dataset FB13k ...\n",
      "brier_score         : 0.212360\n",
      "negative_log_loss   : 0.616571\n",
      "ks_error            : 0.028312\n",
      "training calibrator BetaCalibrator for  model TransE and dataset FB13k ...\n",
      "brier_score         : 0.209246\n",
      "negative_log_loss   : 0.609918\n",
      "ks_error            : 0.018624\n",
      "training calibrator IsotonicCalibrator for  model TransE and dataset FB13k ...\n",
      "brier_score         : 0.205913\n",
      "negative_log_loss   : 0.597525\n",
      "ks_error            : 0.004375\n",
      "training calibrator HistogramBinningCalibtator for  model TransE and dataset FB13k ...\n",
      "brier_score         : 0.216077\n",
      "negative_log_loss   : 0.620413\n",
      "ks_error            : 0.003398\n",
      "                      Uncal     Platt      Beta  Isotonic  HistogramBinning\n",
      "brier_score        0.242040  0.212360  0.209246  0.205913          0.216077\n",
      "negative_log_loss  0.676134  0.616571  0.609918  0.597525          0.620413\n",
      "ks_error           0.098655  0.028312  0.018624  0.004375          0.003398\n",
      "\n",
      "----------------------------------\n",
      "\n",
      "training calibrator UncalCalibtator for  model TransE and dataset WN11 ...\n",
      "brier_score         : 0.242319\n",
      "negative_log_loss   : 0.702034\n",
      "ks_error            : 0.310357\n",
      "training calibrator PlattCalibrator for  model TransE and dataset WN11 ...\n",
      "brier_score         : 0.090351\n",
      "negative_log_loss   : 0.308567\n",
      "ks_error            : 0.024178\n",
      "training calibrator BetaCalibrator for  model TransE and dataset WN11 ...\n",
      "brier_score         : 0.090106\n",
      "negative_log_loss   : 0.309011\n",
      "ks_error            : 0.018784\n",
      "training calibrator IsotonicCalibrator for  model TransE and dataset WN11 ...\n",
      "brier_score         : 0.087382\n",
      "negative_log_loss   : 0.298512\n",
      "ks_error            : 0.004423\n",
      "training calibrator HistogramBinningCalibtator for  model TransE and dataset WN11 ...\n",
      "brier_score         : 0.088269\n",
      "negative_log_loss   : 0.301354\n",
      "ks_error            : 0.005896\n",
      "                      Uncal     Platt      Beta  Isotonic  HistogramBinning\n",
      "brier_score        0.242319  0.090351  0.090106  0.087382          0.088269\n",
      "negative_log_loss  0.702034  0.308567  0.309011  0.298512          0.301354\n",
      "ks_error           0.310357  0.024178  0.018784  0.004423          0.005896\n",
      "\n",
      "----------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "from calmetrics import negative_log_loss\n",
    "\n",
    "def eval_model(model, dataset, calibrators, func_metrics):\n",
    "    df = pd.DataFrame(0, columns=[i.name[:-10] for i in calibrators], index=[i.__name__ for i in func_metrics])\n",
    "\n",
    "    scores_valid = expit(model.predict(dataset.X_valid))\n",
    "    scores_test = expit(model.predict(dataset.X_test))\n",
    "    for calibrator in calibrators:\n",
    "        print(f'training calibrator {calibrator.name} for  model {model.__class__.__name__} and dataset {dataset.name} ...')\n",
    "        calibrator.fit(scores_valid, dataset.y_valid)\n",
    "        probs_test = calibrator.predict_proba(scores_test)\n",
    "#         print(np.max(probs_test))\n",
    "        for func_metric in func_metrics:\n",
    "            print('{:<20}: {:6f}'.format(func_metric.__name__, func_metric(dataset.y_test, probs_test)))\n",
    "            df.loc[func_metric.__name__, calibrator.name[:-10]] = func_metric(dataset.y_test, probs_test)\n",
    "    print(df)\n",
    "    print('\\n----------------------------------\\n')\n",
    "\n",
    "datasets = load_datasets()\n",
    "calibrators = load_calibrators()\n",
    "func_metrics = load_metrics()\n",
    "model_names = os.listdir('10-14_18-38-45')\n",
    "models = {}\n",
    "for name in model_names:\n",
    "    model = restore_model(os.path.join('10-14_18-38-45',name))\n",
    "    models[name] = model\n",
    "\n",
    "\n",
    "blank_calibrators = deepcopy(calibrators)\n",
    "eval_model(models['RandomBaseline-FB13k.pkl'], datasets[0], blank_calibrators, func_metrics)\n",
    "blank_calibrators = deepcopy(calibrators)\n",
    "eval_model(models['RandomBaseline-WN11.pkl'], datasets[1], blank_calibrators, func_metrics)\n",
    "\n",
    "blank_calibrators = deepcopy(calibrators)\n",
    "eval_model(models['TransE-FB13k.pkl'], datasets[0], blank_calibrators, func_metrics)\n",
    "blank_calibrators = deepcopy(calibrators)\n",
    "eval_model(models['TransE-WN11.pkl'], datasets[1], blank_calibrators, func_metrics)\n",
    "\n",
    "blank_calibrators = deepcopy(calibrators)\n",
    "eval_model(models['ComplEx-FB13k.pkl'], datasets[0], blank_calibrators, func_metrics)\n",
    "blank_calibrators = deepcopy(calibrators)\n",
    "eval_model(models['ComplEx-WN11.pkl'], datasets[1], blank_calibrators, func_metrics)\n",
    "\n",
    "blank_calibrators = deepcopy(calibrators)\n",
    "eval_model(models['DistMult-FB13k.pkl'], datasets[0], blank_calibrators, func_metrics)\n",
    "blank_calibrators = deepcopy(calibrators)\n",
    "eval_model(models['DistMult-WN11.pkl'], datasets[1], blank_calibrators, func_metrics)\n",
    "\n",
    "blank_calibrators = deepcopy(calibrators)\n",
    "eval_model(models['HolE-FB13k.pkl'], datasets[0], blank_calibrators, func_metrics)\n",
    "blank_calibrators = deepcopy(calibrators)\n",
    "eval_model(models['HolE-WN11.pkl'], datasets[1], blank_calibrators, func_metrics)\n",
    "\n",
    "blank_calibrators = deepcopy(calibrators)\n",
    "eval_model(models['TransE-FB13k.pkl'], datasets[0], blank_calibrators, func_metrics)\n",
    "blank_calibrators = deepcopy(calibrators)\n",
    "eval_model(models['TransE-WN11.pkl'], datasets[1], blank_calibrators, func_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c6d6b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7c28dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1980b573",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models['TransE-WN11.pkl']\n",
    "dataset = datasets[1]\n",
    "scores_valid = model.predict(dataset.X_valid)\n",
    "scores_test = model.predict(dataset.X_test)\n",
    "\n",
    "print(scores_valid.shape,dataset.y_valid.shape, scores_test.shape)\n",
    "\n",
    "bc = BetaCalibration()\n",
    "bc.fit(scores_valid, dataset.y_valid)\n",
    "\n",
    "bc.calibrator_\n",
    "\n",
    "# bc.predict(scores_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2b5751",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bc.calibrator_.lr_.coef_, bc.calibrator_.lr_.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfa08a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import indexable, column_or_1d\n",
    "from scipy.optimize import minimize_scalar\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def _beta_calibration(df, y, sample_weight=None):\n",
    "#     warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    df = column_or_1d(df).reshape(-1, 1)\n",
    "    eps = np.finfo(df.dtype).eps\n",
    "    df = np.clip(df, eps, 1-eps)\n",
    "    y = column_or_1d(y)\n",
    "\n",
    "    x = np.hstack((df, 1. - df))\n",
    "    x = np.log(x)\n",
    "    x[:, 1] *= -1\n",
    "\n",
    "    lr = LogisticRegression(C=99999999999)\n",
    "    lr.fit(x, y, sample_weight)\n",
    "    coefs = lr.coef_[0]\n",
    "\n",
    "    if coefs[0] < 0:\n",
    "        x = x[:, 1].reshape(-1, 1)\n",
    "        lr = LogisticRegression(C=99999999999)\n",
    "        lr.fit(x, y, sample_weight)\n",
    "        coefs = lr.coef_[0]\n",
    "        a = 0\n",
    "        b = coefs[0]\n",
    "    elif coefs[1] < 0:\n",
    "        x = x[:, 0].reshape(-1, 1)\n",
    "        lr = LogisticRegression(C=99999999999)\n",
    "        lr.fit(x, y, sample_weight)\n",
    "        coefs = lr.coef_[0]\n",
    "        a = coefs[0]\n",
    "        b = 0\n",
    "    else:\n",
    "        a = coefs[0]\n",
    "        b = coefs[1]\n",
    "    inter = lr.intercept_[0]\n",
    "\n",
    "    m = minimize_scalar(lambda mh: np.abs(b*np.log(1.-mh)-a*np.log(mh)-inter),\n",
    "                        bounds=[0, 1], method='Bounded').x\n",
    "    map = [a, b, m]\n",
    "    return map, lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73885788",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp, lr = _beta_calibration(scores_valid, dataset.y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a643a8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install netcal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5a863b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from netcal.binning import HistogramBinning\n",
    "from scipy.special import expit\n",
    "hb = HistogramBinning()\n",
    "hb.fit(expit(scores_valid), dataset.y_valid)\n",
    "hb.transform(expit(scores_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbbb641d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model TransE for dataset FB13k ...\n",
      "training calibrator UncalCalibtator for  model TransE and dataset FB13k ...\n",
      "brier_score : 0.24201365119664545\n",
      "negative_log_loss : 0.6762264953464\n",
      "ks_error : 0.09886559679760165\n",
      "training calibrator PlattCalibrator for  model TransE and dataset FB13k ...\n",
      "brier_score : 0.21211194565765532\n",
      "negative_log_loss : 0.6162166563810462\n",
      "ks_error : 0.027720358046130822\n",
      "training calibrator BetaCalibrator for  model TransE and dataset FB13k ...\n",
      "brier_score : 0.20904371355956375\n",
      "negative_log_loss : 0.6097185854879732\n",
      "ks_error : 0.016289827732570206\n",
      "training calibrator IsotonicCalibrator for  model TransE and dataset FB13k ...\n",
      "brier_score : 0.2063253658975446\n",
      "negative_log_loss : 0.6006230134639148\n",
      "ks_error : 0.004463001221704704\n",
      "training calibrator HistogramBinningCalibtator for  model TransE and dataset FB13k ...\n",
      "brier_score : 0.2153937272397758\n",
      "negative_log_loss : 0.6188629822332721\n",
      "ks_error : 0.004041396131679059\n"
     ]
    }
   ],
   "source": [
    "\n",
    "new_transE = TransE()\n",
    "calibrators = load_calibrators()\n",
    "func_metrics = load_metrics()\n",
    "train_and_eval(new_transE, datasets[0], calibrators, func_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14f561dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average ComplEx Loss:   0.171940: 100%|███████████████████████████████████████████| 100/100 [05:18<00:00,  3.19s/epoch]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64d7abcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.8861175, 6.958498 , 7.050413 , ..., 5.903042 , 4.6821303,\n",
       "       7.8753867], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "03979b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.special import expit\n",
    "from ampligraph.latent_features import ComplEx, TransE, DistMult, HolE\n",
    "from ampligraph.evaluation import generate_corruptions_for_fit, create_mappings, to_idx\n",
    "from netcal.scaling import BetaCalibration \n",
    "from netcal.binning import IsotonicRegression, HistogramBinning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, log_loss\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7ab6aa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_sampling(dat, eta=1):\n",
    "    '''\n",
    "    Given a set of positive triples, perform negative sampling by corruption\n",
    "    return a set of (X, y)\n",
    "    '''\n",
    "    rel_to_idx, ent_to_idx = create_mappings(dat)\n",
    "    dat_id = to_idx(dat, ent_to_idx, rel_to_idx)\n",
    "    dat2 = generate_corruptions_for_fit(dat_id, eta=eta)\n",
    "\n",
    "    dat2_id = dat2.eval(session=tf.compat.v1.Session())\n",
    "    X = np.concatenate([dat_id, dat2_id])\n",
    "    y = np.concatenate([np.ones(len(dat_id)), np.zeros(len(dat2_id))])\n",
    "    idx_to_rel = {v:k for k,v in rel_to_idx.items()}\n",
    "    idx_to_ent = {v:k for k,v in ent_to_idx.items()}\n",
    "    C = []\n",
    "    for x in X:\n",
    "        h, r, t = x\n",
    "        C.append([idx_to_ent[h], idx_to_rel[r], idx_to_ent[t]])\n",
    "\n",
    "    return np.array(C), y\n",
    "\n",
    "def filter_unseen(train, test, y_test):\n",
    "    entities = set()\n",
    "    relations = set()\n",
    "    for x in train:\n",
    "        h, r, t = x\n",
    "        entities.add(h)\n",
    "        relations.add(r)\n",
    "        entities.add(t)\n",
    "    filtered_triple = []\n",
    "    filtered_y = []\n",
    "    for i in range(len(test)):\n",
    "        h, r, t = test[i]\n",
    "        if (h in entities) and (t in entities) and (r in relations):\n",
    "            filtered_triple.append([h, r, t])\n",
    "            filtered_y.append(y_test[i])\n",
    "    return np.array(filtered_triple), np.array(filtered_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "5bb69fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<http://treat.net/onto.owl#alm_100001_248>'\n",
      "  '<http://www.w3.org/1999/02/22-rdf-syntax-ns#type>'\n",
      "  '<http://www.w3.org/2002/07/owl#NamedIndividual>']\n",
      " ['<http://treat.net/onto.owl#alm_100001_363>'\n",
      "  '<http://treat.net/onto.owl#has_parameter>'\n",
      "  '<http://treat.net/onto.owl#pod_15>']\n",
      " ['<http://treat.net/onto.owl#alm_5521_829>'\n",
      "  '<http://treat.net/onto.owl#has_parameter>'\n",
      "  '<http://treat.net/onto.owl#alarm_para_instance_1154>']\n",
      " ...\n",
      " ['<http://treat.net/onto.owl#alm_5521_428>'\n",
      "  '<http://treat.net/onto.owl#has_property>'\n",
      "  '<http://treat.net/onto.owl#alarm_type_6>']\n",
      " ['<http://treat.net/onto.owl#microservice_12>'\n",
      "  '<http://www.w3.org/1999/02/22-rdf-syntax-ns#type>'\n",
      "  '<http://treat.net/onto.owl#Microservice>']\n",
      " ['<http://treat.net/onto.owl#alarm_para_instance_1747>'\n",
      "  '<http://treat.net/onto.owl#has_parameter>'\n",
      "  '<http://treat.net/onto.owl#alarm_para_instance_3026>']] [1. 0. 1. ... 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "with open('ontology_log_instance.ttl', 'r') as f:\n",
    "    lines = np.array([i.strip().split(' ')[:3] for i in f.readlines()])\n",
    "dat = []\n",
    "for l in lines:\n",
    "    if str(l[2]).startswith('<http'):\n",
    "        dat.append(l)\n",
    "dat = np.array(dat)\n",
    "np.random.shuffle(dat)\n",
    "\n",
    "X_train = dat[:int(len(dat)*0.7)]\n",
    "X_rest = dat[int(len(dat)*0.7):]\n",
    "\n",
    "X, y = negative_sampling(X_rest, eta=1)\n",
    "X_cal, X_test, y_cal, y_test = train_test_split(X, y)\n",
    "\n",
    "X_cal, y_cal = filter_unseen(X_train, X_cal, y_cal)\n",
    "X_test, y_test = filter_unseen(X_train, X_test, y_test)\n",
    "\n",
    "print(X_cal, y_cal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "4afd9e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average ComplEx Loss:   0.265538: 100%|███████████████████████████████████████████| 100/100 [02:49<00:00,  1.70s/epoch]\n"
     ]
    }
   ],
   "source": [
    "model = ComplEx(verbose=True)\n",
    "model.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "8bf38b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15712680772841758\n",
      "0.015924066096040586\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>head</th>\n",
       "      <th>relation</th>\n",
       "      <th>tail</th>\n",
       "      <th>proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;http://treat.net/onto.owl#alm_5521_790&gt;</td>\n",
       "      <td>&lt;http://treat.net/onto.owl#has_parameter&gt;</td>\n",
       "      <td>&lt;http://treat.net/onto.owl#alarm_para_instance...</td>\n",
       "      <td>0.14925373134328357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;http://treat.net/onto.owl#alm_5558&gt;</td>\n",
       "      <td>&lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt;</td>\n",
       "      <td>&lt;http://treat.net/onto.owl#alm_5521_661&gt;</td>\n",
       "      <td>0.23404255319148937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;http://treat.net/onto.owl#alm_82013&gt;</td>\n",
       "      <td>&lt;http://treat.net/onto.owl#has_parameter&gt;</td>\n",
       "      <td>&lt;http://treat.net/onto.owl#alarm_para_instance...</td>\n",
       "      <td>0.21661468670258013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;http://treat.net/onto.owl#alm_100001_607&gt;</td>\n",
       "      <td>&lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt;</td>\n",
       "      <td>&lt;http://treat.net/onto.owl#alarm_para_instance...</td>\n",
       "      <td>0.22044728434504793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;http://treat.net/onto.owl#alarm_para_instance...</td>\n",
       "      <td>&lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt;</td>\n",
       "      <td>&lt;http://www.w3.org/2002/07/owl#NamedIndividual&gt;</td>\n",
       "      <td>0.7461257976298997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5311</th>\n",
       "      <td>&lt;http://treat.net/onto.owl#alm_5521_331&gt;</td>\n",
       "      <td>&lt;http://treat.net/onto.owl#has_parameter&gt;</td>\n",
       "      <td>&lt;http://treat.net/onto.owl#alarm_para_instance...</td>\n",
       "      <td>0.7688066167782593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5312</th>\n",
       "      <td>&lt;http://treat.net/onto.owl#alarm_para_instance...</td>\n",
       "      <td>&lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt;</td>\n",
       "      <td>&lt;http://www.w3.org/2002/07/owl#NamedIndividual&gt;</td>\n",
       "      <td>0.7209302325581395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5313</th>\n",
       "      <td>&lt;http://treat.net/onto.owl#alm_100058_15648&gt;</td>\n",
       "      <td>&lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt;</td>\n",
       "      <td>&lt;http://treat.net/onto.owl#alarm_para_instance...</td>\n",
       "      <td>0.3649216161634274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5314</th>\n",
       "      <td>&lt;http://treat.net/onto.owl#alm_100155_83&gt;</td>\n",
       "      <td>&lt;http://treat.net/onto.owl#has_parameter&gt;</td>\n",
       "      <td>&lt;http://treat.net/onto.owl#alm_100001_539&gt;</td>\n",
       "      <td>0.19881305637982197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5315</th>\n",
       "      <td>&lt;http://treat.net/onto.owl#alm_5521_147&gt;</td>\n",
       "      <td>&lt;http://treat.net/onto.owl#has_parameter&gt;</td>\n",
       "      <td>&lt;http://treat.net/onto.owl#alarm_para_instance...</td>\n",
       "      <td>0.21661468670258013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5316 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   head  ...                proba\n",
       "0              <http://treat.net/onto.owl#alm_5521_790>  ...  0.14925373134328357\n",
       "1                  <http://treat.net/onto.owl#alm_5558>  ...  0.23404255319148937\n",
       "2                 <http://treat.net/onto.owl#alm_82013>  ...  0.21661468670258013\n",
       "3            <http://treat.net/onto.owl#alm_100001_607>  ...  0.22044728434504793\n",
       "4     <http://treat.net/onto.owl#alarm_para_instance...  ...   0.7461257976298997\n",
       "...                                                 ...  ...                  ...\n",
       "5311           <http://treat.net/onto.owl#alm_5521_331>  ...   0.7688066167782593\n",
       "5312  <http://treat.net/onto.owl#alarm_para_instance...  ...   0.7209302325581395\n",
       "5313       <http://treat.net/onto.owl#alm_100058_15648>  ...   0.3649216161634274\n",
       "5314          <http://treat.net/onto.owl#alm_100155_83>  ...  0.19881305637982197\n",
       "5315           <http://treat.net/onto.owl#alm_5521_147>  ...  0.21661468670258013\n",
       "\n",
       "[5316 rows x 4 columns]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_cal = expit(model.predict(X_cal))\n",
    "calibrator = IsotonicRegression()\n",
    "calibrator.fit(scores_cal, y_cal)\n",
    "scores_test = expit(model.predict(X_test))\n",
    "proba_test = calibrator.transform(scores_test)\n",
    "\n",
    "print(mean_squared_error(proba_test, y_test))\n",
    "ece = ECE()\n",
    "print(ece.measure(proba_test, y_test))\n",
    "\n",
    "prob_triples = np.c_[(X_test, proba_test)]\n",
    "df = pd.DataFrame(prob_triples,columns=['head', 'relation', 'tail', 'proba'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "244da9e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>head</th>\n",
       "      <th>relation</th>\n",
       "      <th>tail</th>\n",
       "      <th>proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;http://treat.net/onto.owl#alm_5521_790&gt;</td>\n",
       "      <td>&lt;http://treat.net/onto.owl#has_parameter&gt;</td>\n",
       "      <td>&lt;http://treat.net/onto.owl#alarm_para_instance...</td>\n",
       "      <td>0.14925373134328357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;http://treat.net/onto.owl#alm_5558&gt;</td>\n",
       "      <td>&lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt;</td>\n",
       "      <td>&lt;http://treat.net/onto.owl#alm_5521_661&gt;</td>\n",
       "      <td>0.23404255319148937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;http://treat.net/onto.owl#alm_82013&gt;</td>\n",
       "      <td>&lt;http://treat.net/onto.owl#has_parameter&gt;</td>\n",
       "      <td>&lt;http://treat.net/onto.owl#alarm_para_instance...</td>\n",
       "      <td>0.21661468670258013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;http://treat.net/onto.owl#alm_100001_607&gt;</td>\n",
       "      <td>&lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt;</td>\n",
       "      <td>&lt;http://treat.net/onto.owl#alarm_para_instance...</td>\n",
       "      <td>0.22044728434504793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;http://treat.net/onto.owl#alarm_para_instance...</td>\n",
       "      <td>&lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt;</td>\n",
       "      <td>&lt;http://www.w3.org/2002/07/owl#NamedIndividual&gt;</td>\n",
       "      <td>0.7461257976298997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5311</th>\n",
       "      <td>&lt;http://treat.net/onto.owl#alm_5521_331&gt;</td>\n",
       "      <td>&lt;http://treat.net/onto.owl#has_parameter&gt;</td>\n",
       "      <td>&lt;http://treat.net/onto.owl#alarm_para_instance...</td>\n",
       "      <td>0.7688066167782593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5312</th>\n",
       "      <td>&lt;http://treat.net/onto.owl#alarm_para_instance...</td>\n",
       "      <td>&lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt;</td>\n",
       "      <td>&lt;http://www.w3.org/2002/07/owl#NamedIndividual&gt;</td>\n",
       "      <td>0.7209302325581395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5313</th>\n",
       "      <td>&lt;http://treat.net/onto.owl#alm_100058_15648&gt;</td>\n",
       "      <td>&lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt;</td>\n",
       "      <td>&lt;http://treat.net/onto.owl#alarm_para_instance...</td>\n",
       "      <td>0.3649216161634274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5314</th>\n",
       "      <td>&lt;http://treat.net/onto.owl#alm_100155_83&gt;</td>\n",
       "      <td>&lt;http://treat.net/onto.owl#has_parameter&gt;</td>\n",
       "      <td>&lt;http://treat.net/onto.owl#alm_100001_539&gt;</td>\n",
       "      <td>0.19881305637982197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5315</th>\n",
       "      <td>&lt;http://treat.net/onto.owl#alm_5521_147&gt;</td>\n",
       "      <td>&lt;http://treat.net/onto.owl#has_parameter&gt;</td>\n",
       "      <td>&lt;http://treat.net/onto.owl#alarm_para_instance...</td>\n",
       "      <td>0.21661468670258013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5316 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   head  ...                proba\n",
       "0              <http://treat.net/onto.owl#alm_5521_790>  ...  0.14925373134328357\n",
       "1                  <http://treat.net/onto.owl#alm_5558>  ...  0.23404255319148937\n",
       "2                 <http://treat.net/onto.owl#alm_82013>  ...  0.21661468670258013\n",
       "3            <http://treat.net/onto.owl#alm_100001_607>  ...  0.22044728434504793\n",
       "4     <http://treat.net/onto.owl#alarm_para_instance...  ...   0.7461257976298997\n",
       "...                                                 ...  ...                  ...\n",
       "5311           <http://treat.net/onto.owl#alm_5521_331>  ...   0.7688066167782593\n",
       "5312  <http://treat.net/onto.owl#alarm_para_instance...  ...   0.7209302325581395\n",
       "5313       <http://treat.net/onto.owl#alm_100058_15648>  ...   0.3649216161634274\n",
       "5314          <http://treat.net/onto.owl#alm_100155_83>  ...  0.19881305637982197\n",
       "5315           <http://treat.net/onto.owl#alm_5521_147>  ...  0.21661468670258013\n",
       "\n",
       "[5316 rows x 4 columns]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "54c75854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['proba'].astype(float).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "654ef46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15048169148345975\n",
      "0.31942629540009115\n"
     ]
    }
   ],
   "source": [
    "scores_test = expit(model.predict(X_test[y_test==1]))\n",
    "proba_test = calibrator.transform(scores_test)\n",
    "\n",
    "print(mean_squared_error(proba_test, y_test[y_test==1]))\n",
    "ece = ECE()\n",
    "print(ece.measure(proba_test, y_test[y_test==1]))\n",
    "\n",
    "prob_triples = np.c_[(X_test[y_test==1], proba_test)]\n",
    "df = pd.DataFrame(prob_triples,columns=['head', 'relation', 'tail', 'proba'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "ba806bca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>head</th>\n",
       "      <th>relation</th>\n",
       "      <th>tail</th>\n",
       "      <th>proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;http://treat.net/onto.owl#alm_5521_790&gt;</td>\n",
       "      <td>&lt;http://treat.net/onto.owl#has_parameter&gt;</td>\n",
       "      <td>&lt;http://treat.net/onto.owl#alarm_para_instance...</td>\n",
       "      <td>0.14925373134328357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;http://treat.net/onto.owl#alarm_para_instance...</td>\n",
       "      <td>&lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt;</td>\n",
       "      <td>&lt;http://www.w3.org/2002/07/owl#NamedIndividual&gt;</td>\n",
       "      <td>0.7461257976298997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;http://treat.net/onto.owl#cell_service_27&gt;</td>\n",
       "      <td>&lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt;</td>\n",
       "      <td>&lt;http://www.w3.org/2002/07/owl#NamedIndividual&gt;</td>\n",
       "      <td>0.363395225464191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;http://treat.net/onto.owl#alm_100001_260&gt;</td>\n",
       "      <td>&lt;http://treat.net/onto.owl#has_parameter&gt;</td>\n",
       "      <td>&lt;http://treat.net/onto.owl#alarm_para_instance...</td>\n",
       "      <td>0.22044728434504793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;http://treat.net/onto.owl#alarm_para_instance...</td>\n",
       "      <td>&lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt;</td>\n",
       "      <td>&lt;http://www.w3.org/2002/07/owl#NamedIndividual&gt;</td>\n",
       "      <td>0.7461257976298997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2605</th>\n",
       "      <td>&lt;http://treat.net/onto.owl#alm_5521_479&gt;</td>\n",
       "      <td>&lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt;</td>\n",
       "      <td>&lt;http://www.w3.org/2002/07/owl#NamedIndividual&gt;</td>\n",
       "      <td>0.7461257976298997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2606</th>\n",
       "      <td>&lt;http://treat.net/onto.owl#alm_100058_15571&gt;</td>\n",
       "      <td>&lt;http://treat.net/onto.owl#has_parameter&gt;</td>\n",
       "      <td>&lt;http://treat.net/onto.owl#alarm_para_instance...</td>\n",
       "      <td>0.7688066167782593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2607</th>\n",
       "      <td>&lt;http://treat.net/onto.owl#alm_5521_331&gt;</td>\n",
       "      <td>&lt;http://treat.net/onto.owl#has_parameter&gt;</td>\n",
       "      <td>&lt;http://treat.net/onto.owl#alarm_para_instance...</td>\n",
       "      <td>0.7688066167782593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2608</th>\n",
       "      <td>&lt;http://treat.net/onto.owl#alarm_para_instance...</td>\n",
       "      <td>&lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt;</td>\n",
       "      <td>&lt;http://www.w3.org/2002/07/owl#NamedIndividual&gt;</td>\n",
       "      <td>0.7209302325581395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2609</th>\n",
       "      <td>&lt;http://treat.net/onto.owl#alm_5521_147&gt;</td>\n",
       "      <td>&lt;http://treat.net/onto.owl#has_parameter&gt;</td>\n",
       "      <td>&lt;http://treat.net/onto.owl#alarm_para_instance...</td>\n",
       "      <td>0.21661468670258013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2610 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   head  ...                proba\n",
       "0              <http://treat.net/onto.owl#alm_5521_790>  ...  0.14925373134328357\n",
       "1     <http://treat.net/onto.owl#alarm_para_instance...  ...   0.7461257976298997\n",
       "2           <http://treat.net/onto.owl#cell_service_27>  ...    0.363395225464191\n",
       "3            <http://treat.net/onto.owl#alm_100001_260>  ...  0.22044728434504793\n",
       "4     <http://treat.net/onto.owl#alarm_para_instance...  ...   0.7461257976298997\n",
       "...                                                 ...  ...                  ...\n",
       "2605           <http://treat.net/onto.owl#alm_5521_479>  ...   0.7461257976298997\n",
       "2606       <http://treat.net/onto.owl#alm_100058_15571>  ...   0.7688066167782593\n",
       "2607           <http://treat.net/onto.owl#alm_5521_331>  ...   0.7688066167782593\n",
       "2608  <http://treat.net/onto.owl#alarm_para_instance...  ...   0.7209302325581395\n",
       "2609           <http://treat.net/onto.owl#alm_5521_147>  ...  0.21661468670258013\n",
       "\n",
       "[2610 rows x 4 columns]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "97c6b709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['proba'].astype(float).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "8c1525b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR - Input triples include one or more concepts not present in the training set. Please filter all concepts in X that do not occur in the training test (set filter_unseen=True in evaluate_performance) or retrain the model on a training set that includes all the desired concept types.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input triples include one or more concepts not present in the training set. Please filter all concepts in X that do not occur in the training test (set filter_unseen=True in evaluate_performance) or retrain the model on a training set that includes all the desired concept types.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mF:\\codes\\misc\\kgebd\\.venv\\lib\\site-packages\\ampligraph\\evaluation\\protocol.py\u001b[0m in \u001b[0;36m_convert_to_idx\u001b[1;34m(X, ent_to_idx, rel_to_idx, obj_to_idx)\u001b[0m\n\u001b[0;32m    647\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 648\u001b[1;33m         \u001b[0mx_idx_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ment_to_idx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    649\u001b[0m         \u001b[0mx_idx_p\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrel_to_idx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\codes\\misc\\kgebd\\.venv\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2090\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2091\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vectorize_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2092\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\codes\\misc\\kgebd\\.venv\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36m_vectorize_call\u001b[1;34m(self, func, args)\u001b[0m\n\u001b[0;32m   2169\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnout\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2170\u001b[1;33m                 \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0motypes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2171\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'NoneType'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\RUIQIZ~1\\AppData\\Local\\Temp/ipykernel_8608/3354062688.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mX_dat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0my_dat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_dat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mscores_dat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexpit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_dat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mproba_dat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalibrator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores_dat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\codes\\misc\\kgebd\\.venv\\lib\\site-packages\\ampligraph\\latent_features\\models\\ComplEx.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, from_idx)\u001b[0m\n\u001b[0;32m    375\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrom_idx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[0m__doc__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m  \u001b[1;31m# NOQA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 377\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrom_idx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfrom_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    378\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcalibrate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_pos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_neg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpositive_base_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatches_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\codes\\misc\\kgebd\\.venv\\lib\\site-packages\\ampligraph\\latent_features\\models\\EmbeddingModel.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, from_idx)\u001b[0m\n\u001b[0;32m   1727\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdealing_with_large_graphs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1728\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfrom_idx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1729\u001b[1;33m                 \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_idx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ment_to_idx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ment_to_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrel_to_idx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrel_to_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1730\u001b[0m             \u001b[0mx_tf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1731\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\codes\\misc\\kgebd\\.venv\\lib\\site-packages\\ampligraph\\evaluation\\protocol.py\u001b[0m in \u001b[0;36mto_idx\u001b[1;34m(X, ent_to_idx, rel_to_idx)\u001b[0m\n\u001b[0;32m    686\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 688\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_convert_to_idx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ment_to_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrel_to_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ment_to_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\codes\\misc\\kgebd\\.venv\\lib\\site-packages\\ampligraph\\evaluation\\protocol.py\u001b[0m in \u001b[0;36m_convert_to_idx\u001b[1;34m(X, ent_to_idx, rel_to_idx, obj_to_idx)\u001b[0m\n\u001b[0;32m    652\u001b[0m         \u001b[0munseen_msg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munseen_msg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'concept_type'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'concepts'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    653\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munseen_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 654\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munseen_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    655\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    656\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx_idx_s\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx_idx_o\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input triples include one or more concepts not present in the training set. Please filter all concepts in X that do not occur in the training test (set filter_unseen=True in evaluate_performance) or retrain the model on a training set that includes all the desired concept types."
     ]
    }
   ],
   "source": [
    "X_dat = dat\n",
    "y_dat = np.ones(len(X_dat))\n",
    "scores_dat = expit(model.predict(X_dat))\n",
    "proba_dat = calibrator.transform(scores_dat)\n",
    "\n",
    "print(mean_squared_error(proba_test, y_dat))\n",
    "ece = ECE()\n",
    "print(ece.measure(proba_test, y_test[y_test==1]))\n",
    "\n",
    "prob_triples = np.c_[(X_dat, proba_dat)]\n",
    "df = pd.DataFrame(prob_triples,columns=['head', 'relation', 'tail', 'proba'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4f0cf7ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "df['proba'].astype(float).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "846c4911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD4CAYAAAAO9oqkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAASe0lEQVR4nO3cf6xcd3nn8fenMaEsgSZgehUlbp1V3VUN1kK4Sryi2p00VeIECadahBIF4tAsriBZtbvWCrerVRABKaiFSpFoWiMsnFWLydJ2YxFTy8pmFLGq0zglzS+Wzd1gGnvTZMEm1KQLNX36x3ydHd2d6ztz7TvjzH2/pNGcec75nvN9PJY/PmfOTKoKSdLK9hOTnoAkafIMA0mSYSBJMgwkSRgGkiRg1aQnsFSrV6+utWvXjjTmBz/4Aa9//euXZ0JnOXtfeb2v1L7B3k/V+6OPPvqdqnrL/PqrNgzWrl3LwYMHRxrT7XbpdDrLM6GznL13Jj2NsVupfYO9n6r3JN8eVPcykSTJMJAkGQaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSeBV/A1nS2WPt9vsnctxDd757IsedRp4ZSJIMA0mSYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRJDhEGSNUkeTPJ0kqeS/HqrfyzJkSSPtce1fWN+M8lckm8mubqvvqnV5pJs76tfkuThVv9SknPPdKOSpIUNc2ZwAthWVeuBjcCtSda3db9bVW9vj70Abd31wFuBTcDvJTknyTnAZ4FrgPXADX37+VTb188Bx4BbzlB/kqQhLBoGVfV8Vf1lW/5b4BvARacYshnYXVU/rKpvAXPAZe0xV1XPVtWPgN3A5iQBfgn4chu/C7huif1IkpZgpN8mSrIWeAfwMPAu4LYkNwEH6Z09HKMXFAf6hh3m/4XHc/PqlwNvBr5XVScGbD//+FuBrQAzMzN0u91Rps/x48dHHjMt7L076WmM3Tj73rbhxOIbLYOF+lup7zksvfehwyDJecAfA79RVd9PcjdwB1Dt+dPAr448gxFU1Q5gB8Ds7Gx1Op2Rxne7XUYdMy3svTPpaYzdOPu+eVI/VHdjZ2B9pb7nsPTehwqDJK+hFwR/WFV/AlBVL/St/xzwlfbyCLCmb/jFrcYC9e8C5ydZ1c4O+reXJI3BMHcTBfg88I2q+kxf/cK+zX4FeLIt7wGuT/LaJJcA64C/AB4B1rU7h86l9yHznqoq4EHgvW38FuC+02tLkjSKYc4M3gV8AHgiyWOt9lv07gZ6O73LRIeAXwOoqqeS3As8Te9OpFur6scASW4D9gHnADur6qm2v48Cu5N8Avg6vfCRJI3JomFQVV8DMmDV3lOM+STwyQH1vYPGVdWz9O42kiRNgN9AliQZBpIkw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSGCIMkqxJ8mCSp5M8leTXW/1NSfYneaY9X9DqSXJXkrkkjye5tG9fW9r2zyTZ0ld/Z5In2pi7kmQ5mpUkDTbMmcEJYFtVrQc2ArcmWQ9sBx6oqnXAA+01wDXAuvbYCtwNvfAAbgcuBy4Dbj8ZIG2bD/WN23T6rUmShrVoGFTV81X1l235b4FvABcBm4FdbbNdwHVteTNwT/UcAM5PciFwNbC/qo5W1TFgP7CprXtjVR2oqgLu6duXJGkMVo2ycZK1wDuAh4GZqnq+rfobYKYtXwQ81zfscKudqn54QH3Q8bfSO9tgZmaGbrc7yvQ5fvz4yGOmhb13Jz2NsRtn39s2nBjLceZbqL+V+p7D0nsfOgySnAf8MfAbVfX9/sv6VVVJauSjj6iqdgA7AGZnZ6vT6Yw0vtvtMuqYaWHvnUlPY+zG2ffN2+8fy3HmO3RjZ2B9pb7nsPTeh7qbKMlr6AXBH1bVn7TyC+0SD+35xVY/AqzpG35xq52qfvGAuiRpTIa5myjA54FvVNVn+lbtAU7eEbQFuK+vflO7q2gj8FK7nLQPuCrJBe2D46uAfW3d95NsbMe6qW9fkqQxGOYy0buADwBPJHms1X4LuBO4N8ktwLeB97V1e4FrgTngZeCDAFV1NMkdwCNtu49X1dG2/BHgC8DrgK+2hyRpTBYNg6r6GrDQff9XDti+gFsX2NdOYOeA+kHgbYvNRZK0PPwGsiTJMJAkGQaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CSxBBhkGRnkheTPNlX+1iSI0kea49r+9b9ZpK5JN9McnVffVOrzSXZ3le/JMnDrf6lJOeeyQYlSYsb5szgC8CmAfXfraq3t8degCTrgeuBt7Yxv5fknCTnAJ8FrgHWAze0bQE+1fb1c8Ax4JbTaUiSNLpFw6CqHgKODrm/zcDuqvphVX0LmAMua4+5qnq2qn4E7AY2JwnwS8CX2/hdwHWjtSBJOl2rTmPsbUluAg4C26rqGHARcKBvm8OtBvDcvPrlwJuB71XViQHb/3+SbAW2AszMzNDtdkea8PHjx0ceMy3svTvpaYzdOPvetuHE4hstg4X6W6nvOSy996WGwd3AHUC1508Dv7rEfQ2tqnYAOwBmZ2er0+mMNL7b7TLqmGlh751JT2Psxtn3zdvvH8tx5jt0Y2dgfaW+57D03pcUBlX1wsnlJJ8DvtJeHgHW9G16cauxQP27wPlJVrWzg/7tJUljsqRbS5Nc2PfyV4CTdxrtAa5P8toklwDrgL8AHgHWtTuHzqX3IfOeqirgQeC9bfwW4L6lzEmStHSLnhkk+SLQAVYnOQzcDnSSvJ3eZaJDwK8BVNVTSe4FngZOALdW1Y/bfm4D9gHnADur6ql2iI8Cu5N8Avg68Pkz1ZwkaTiLhkFV3TCgvOA/2FX1SeCTA+p7gb0D6s/Su9tIkjQhfgNZkmQYSJIMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEliiDBIsjPJi0me7Ku9Kcn+JM+05wtaPUnuSjKX5PEkl/aN2dK2fybJlr76O5M80cbclSRnuklJ0qkNc2bwBWDTvNp24IGqWgc80F4DXAOsa4+twN3QCw/gduBy4DLg9pMB0rb5UN+4+ceSJC2zRcOgqh4Cjs4rbwZ2teVdwHV99Xuq5wBwfpILgauB/VV1tKqOAfuBTW3dG6vqQFUVcE/fviRJY7JqieNmqur5tvw3wExbvgh4rm+7w612qvrhAfWBkmyld8bBzMwM3W53pEkfP3585DHTwt67k57G2I2z720bTozlOPMt1N9Kfc9h6b0vNQxeUVWVpE53P0MeawewA2B2drY6nc5I47vdLqOOmRb23pn0NMZunH3fvP3+sRxnvkM3dgbWV+p7Dkvvfal3E73QLvHQnl9s9SPAmr7tLm61U9UvHlCXJI3RUsNgD3DyjqAtwH199ZvaXUUbgZfa5aR9wFVJLmgfHF8F7Gvrvp9kY7uL6Ka+fUmSxmTRy0RJvgh0gNVJDtO7K+hO4N4ktwDfBt7XNt8LXAvMAS8DHwSoqqNJ7gAeadt9vKpOfij9EXp3LL0O+Gp7SJLGaNEwqKobFlh15YBtC7h1gf3sBHYOqB8E3rbYPCRJy8dvIEuSDANJkmEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJAGrJj0BSVqqtdvvH1jftuEENy+w7kw5dOe7l3X/4+aZgSTJMJAkGQaSJAwDSRKGgSQJw0CShGEgScIwkCThl86kM26hL0KNw7R9EUrjc1pnBkkOJXkiyWNJDrbam5LsT/JMe76g1ZPkriRzSR5Pcmnffra07Z9JsuX0WpIkjepMnBlcUVXf6Xu9HXigqu5Msr29/ihwDbCuPS4H7gYuT/Im4HZgFijg0SR7qurYGZibJC2LSZ0BLtfZ33J8ZrAZ2NWWdwHX9dXvqZ4DwPlJLgSuBvZX1dEWAPuBTcswL0nSAlJVSx+cfAs4Ru9/9H9QVTuSfK+qzm/rAxyrqvOTfAW4s6q+1tY9QO+MoQP8ZFV9otX/E/B3VfU7A463FdgKMDMz887du3ePNN/jx49z3nnnLanXVzt7H1/vTxx5aWzHmm/DRT/1yvI4+55kz4PMvA5e+LtJz2J59L/Hgyz2vl9xxRWPVtXs/PrpXib6xao6kuSngf1J/kf/yqqqJEtPm3mqagewA2B2drY6nc5I47vdLqOOmRb23hnb8Zb71zJP5dCNnVeWx9n3JHseZNuGE3z6iem8P6b/PR5kqe/7aV0mqqoj7flF4E+By4AX2uUf2vOLbfMjwJq+4Re32kJ1SdKYLDkMkrw+yRtOLgNXAU8Ce4CTdwRtAe5ry3uAm9pdRRuBl6rqeWAfcFWSC9qdR1e1miRpTE7nPGoG+NPexwKsAv6oqv4sySPAvUluAb4NvK9tvxe4FpgDXgY+CFBVR5PcATzStvt4VR09jXlJkka05DCoqmeBfz6g/l3gygH1Am5dYF87gZ1LnYsk6fT4cxSSJMNAkmQYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJwKpJT0DSmbN2+/2vLG/bcIKb+15Lp2IYaGqd/IfRfxSlxZ01l4mSbEryzSRzSbZPej6StJKcFWGQ5Bzgs8A1wHrghiTrJzsrSVo5zpbLRJcBc1X1LECS3cBm4OmJzmqKPHHkpYlcKjl057vHfkxJo0tVTXoOJHkvsKmq/k17/QHg8qq6bd52W4Gt7eU/A7454qFWA985zem+Wtn7yrNS+wZ7P1XvP1tVb5lfPFvODIZSVTuAHUsdn+RgVc2ewSm9atj7yut9pfYN9r6U3s+KzwyAI8CavtcXt5okaQzOljB4BFiX5JIk5wLXA3smPCdJWjHOistEVXUiyW3APuAcYGdVPbUMh1ryJaYpYO8rz0rtG+x9ZGfFB8iSpMk6Wy4TSZImyDCQJE1nGCz20xZJXpvkS239w0nWTmCay2KI3v99kqeTPJ7kgSQ/O4l5Lodhf9Ikyb9OUkmm4tbDYfpO8r72vj+V5I/GPcflMsTf959J8mCSr7e/89dOYp5nWpKdSV5M8uQC65Pkrvbn8niSSxfdaVVN1YPeB9D/C/inwLnAXwHr523zEeD32/L1wJcmPe8x9n4F8E/a8odXUu9tuzcADwEHgNlJz3tM7/k64OvABe31T0963mPsfQfw4ba8Hjg06Xmfod7/JXAp8OQC668FvgoE2Ag8vNg+p/HM4JWftqiqHwEnf9qi32ZgV1v+MnBlkoxxjstl0d6r6sGqerm9PEDvOx3TYJj3HeAO4FPA/x3n5JbRMH1/CPhsVR0DqKoXxzzH5TJM7wW8sS3/FPC/xzi/ZVNVDwFHT7HJZuCe6jkAnJ/kwlPtcxrD4CLgub7Xh1tt4DZVdQJ4CXjzWGa3vIbpvd8t9P73MA0W7b2dKq+pqmn6Peth3vOfB34+yX9PciDJprHNbnkN0/vHgPcnOQzsBf7teKY2caP+W3B2fM9A45fk/cAs8K8mPZdxSPITwGeAmyc8lUlYRe9SUYfemeBDSTZU1fcmOakxuQH4QlV9Osm/AP5zkrdV1T9MemJnm2k8Mxjmpy1e2SbJKnqnj98dy+yW11A/65Hkl4H/CLynqn44prktt8V6fwPwNqCb5BC966h7puBD5GHe88PAnqr6+6r6FvA/6YXDq90wvd8C3AtQVX8O/CS9H3KbdiP/xM80hsEwP22xB9jSlt8L/Ldqn7q8yi3ae5J3AH9ALwim5doxLNJ7Vb1UVauram1VraX3ecl7qurgZKZ7xgzz9/2/0jsrIMlqepeNnh3jHJfLML3/NXAlQJJfoBcG/2ess5yMPcBN7a6ijcBLVfX8qQZM3WWiWuCnLZJ8HDhYVXuAz9M7XZyj9yHM9ZOb8ZkzZO+/DZwH/Jf2mflfV9V7JjbpM2TI3qfOkH3vA65K8jTwY+A/VNWr/kx4yN63AZ9L8u/ofZh88zT8xy/JF+kF/Or2ecjtwGsAqur36X0+ci0wB7wMfHDRfU7Bn4sk6TRN42UiSdKIDANJkmEgSTIMJEkYBpIkDANJEoaBJAn4R5Q9RyfyBSCSAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from netcal.binning import IsotonicRegression\n",
    "isot = IsotonicRegression()\n",
    "s = expit(model.predict(C))\n",
    "isot.fit(s, y)\n",
    "p = isot.transform(expit(model.predict(dat)))\n",
    "prob_t = np.c_[(dat, p)]\n",
    "df2 = pd.DataFrame(prob_t,columns=['head', 'relation', 'tail', 'proba'])\n",
    "df2['proba'].astype(float).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "44fc781f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5a8221",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
